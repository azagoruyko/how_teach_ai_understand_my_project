{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083e1eeb",
   "metadata": {},
   "source": [
    "### Problem\n",
    "How to teach AI to understand my python project?  \n",
    "\n",
    "I want to ask AI questions about the project like:\n",
    "* Where function X is defined and how it's used in the project?\n",
    "* What's the architecture of the project?\n",
    "* How to add new components?\n",
    "* etc.\n",
    "\n",
    "That's my experience I'd like to share with you. Hope you find it interesting to start experimenting with LLMs.\n",
    "\n",
    "### Toolset installation\n",
    "\n",
    "##### Python\n",
    "Download and install Python 3.9 or higher. Then install requirements.\n",
    "\n",
    "```bash\n",
    "pip install -r Requirements.txt\n",
    "```\n",
    "\n",
    "This should install FAISS for vector database, Langchain for LLM based application and other dependencies.\n",
    "\n",
    "##### Ollama\n",
    "Ollama is a framework for running LLMs locally.\n",
    "[Download](https://github.com/ollama/ollama/releases) and install it as usual.\n",
    "\n",
    "Once installed, pull LLM model:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "Make sure the model is downloaded with the command:\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "Ollama is ready!\n",
    "\n",
    "## Let's go!\n",
    "Run the following code to make sure everything is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\") # the model we want to use\n",
    "response = llm.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b3ff",
   "metadata": {},
   "source": [
    "You should get the response from LLM that it's ready for helping you. The cool thing here is that you can pull new models in Ollama and use them right away! \n",
    "\n",
    "You can even use multiple models at the same time.\n",
    "\n",
    "You can browse modules on [Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&num_parameters=min:0,max:12B&sort=trending&search=gguf). It's like github for LLMs. Ollama supports GGUF models.\n",
    "\n",
    "Make sure that models can be very large and it directly affects the memory required to load the model. \n",
    "\n",
    "**I don't recommend experimenting with models larger than 4 GB as it can be very slow and consume a lot of memory.**\n",
    "\n",
    "Ok, let's define some variables for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d757899",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjectFolder = \"./rigBuilder\" # that's a path to your project, use your own path\n",
    "TestFile = \"./rigBuilder/widgets.py\" # a file for testing from the project\n",
    "ProjectInfo = \"Rig Builder is a tool for making UIs for python scripts.\" # something about your project for LLM to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbf025",
   "metadata": {},
   "source": [
    "\n",
    "Here we start working with FAISS. FAISS is a vector database for storing and searching vectors.\n",
    "It converts texts or documents to vectors and stores them in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface  import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "with open(TestFile) as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = Document(page_content=text, metadata={\"source\": TestFile})\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = FAISS.from_documents([doc], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90ccc",
   "metadata": {},
   "source": [
    "`Document` is a class from Langchain that represents a textual document. It has a `page_content` attribute that stores the text of the document and `metadata` attribute that stores additional information about the document like file name, type, etc. It works as a base class for all documents in Langchain.\n",
    "\n",
    "FAISS requires a model to be used for vectorization. We use here `sentence-transformers/all-MiniLM-L6-v2` model. There are lots of them, but once you've chosen one, you cannot change it for the same vector database. For simplicity, understand this model as a function that converts text to a vector (array of numbers).\n",
    "\n",
    "You can then search for similar texts in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(vector_db.similarity_search(\"how to make custom widgets?\")):\n",
    "    print(i, chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f08c1",
   "metadata": {},
   "source": [
    "The problem here is that we have a single file in the database and so it will always return the same document.\n",
    "\n",
    "Of course, we can add more files in the database and make a searcher that finds a file that's more relevant to the question. But it will anyway return the whole document which is redundant and inefficient, not to mention that it won't give us any precise answers regarding the documents.\n",
    "\n",
    "The main idea behind such databases is that we need to split textual data into smaller chunks before storing them. Each chunk is a portion of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccafda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20) # split by text separators and chunk size\n",
    "chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(i, chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bfbfa",
   "metadata": {},
   "source": [
    "Now we have much more documents (pieces of information) in the database. Let's try to search and see if it returns something relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1aae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(vector_db.similarity_search(\"how to make custom widgets?\", k=5)): # k is the number of documents to return\n",
    "    print(i, chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b006f",
   "metadata": {},
   "source": [
    "Now we have much more results, but the textual fragments are very small and often look \"random\". Why so?\n",
    "\n",
    "The reason is that the vector database searches for the semantic meaning (similarity). It doesn't understand the code at all. \n",
    "\n",
    "If you search for `how to make a new component for my project?` it may find chunks with `def make_component` or `my_project.build()`. Yeah, it's not that stupid, but it's not that smart either. You probably don't have such phrases in your codebase like `how to make ...` and so searching will not give any useful information about how to actually create a new component for the project. You'll just get nearest code chunks to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469530d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", ProjectInfo+\"\\nUse the following context to answer the questions.\\nContext: {context}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm # langchain chaining\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5}) # number of chunks to retrieve\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain) # this is a high level chain for retrieving data somewhere\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how to make a new component for my project?\"})\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75db094",
   "metadata": {},
   "source": [
    "That's a bit complicated code but let's discover it step by step.\n",
    "\n",
    "First, prompts. Remember, guys (I'm saying this to myself as well), prompts are the main components for LLMs. It's very important to understand how they work and how they affect LLMs.\n",
    "\n",
    "In Langchain, a prompt is just a template with placeholders for variables (sometimes optional). The idea behind prompt templates is that you can define them once and then reuse many times with different variables. \n",
    "\n",
    "A prompt has roles and messages.\n",
    "\n",
    "| Role | Message |\n",
    "| -- | -- |\n",
    "| system | You're an expert in AI, answer briefly without long descriptions. |\n",
    "| user | What are the prompts for LLMs? |\n",
    "\n",
    "Second, chaining. The following code creates a _chain_ of operations within Langchain framework. \n",
    "```python\n",
    "chain = prompt_template | llm\n",
    "```\n",
    "\n",
    "Simply, this means that you can pass data into `chain` and it will pass it through `prompt_template` and then through `llm`.\n",
    "\n",
    "Each chain has `invoke` method that actually 'executes' the chain.\n",
    "\n",
    "Important thing here is that `create_retrieval_chain` waits for `context` variable to be existed in the prompt.\n",
    "\n",
    "So, how it works?\n",
    "\n",
    "1. You ask a question (it goes to human `{input}` ).\n",
    "2. `create_retrieval_chain` searches for the nearest chunks to the question in the vector database with the retriever.\n",
    "3. Then passes those chunks to the prompt under `context` variable.\n",
    "4. Then everything is combined and`chain` processes the full prompt with the context.\n",
    "5. `chain` calls LLM and returns the answer.\n",
    "\n",
    "Keys like `answer` or `context` can be changed in the functions call as additional arguments if needed.\n",
    "\n",
    "What do we have after the `chain` call? Did the result get better?\n",
    "\n",
    "If you run the code, you maybe found that the result is still quite weird and a bit random. \n",
    "\n",
    "Because the actual prompt to the LLM may look like:\n",
    "```code\n",
    "Use the following context to answer the questions.\n",
    "Context: def make_component():\n",
    "    \"\"\"Make a new component for the project.\"\"\"    \n",
    "    return Component()\n",
    "project.add_component()\n",
    "...\n",
    "how to make a new component for my project?\n",
    "```\n",
    "\n",
    "LLM gets a mess of code chunks and it's not able to find the correct answer!\n",
    "\n",
    "There are number of solutions to this problem. First, split the file more precisely using dedicated python splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4caf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=400, chunk_overlap=100)\n",
    "chunks = code_splitter.split_documents([doc])\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(i, chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad5340",
   "metadata": {},
   "source": [
    "This returns more accurate chunks based on Python syntax. But if you try to run the previous cell, the result won't be much better, unfortunately. You can play with different `chunk_size` and `chunk_overlap` parameters, increase `k` in `as_retriever`, but you will be stuck after some time.\n",
    "\n",
    "This may lead to a question: \"Does LLM really understand what I want?\" because the results are almost random and irrelevant.\n",
    "\n",
    "Here we come to one of the most important things I've learnt regarding LLMs.\n",
    "\n",
    "### CONTEXT\n",
    "\n",
    "Context is limited. If you have lots of files or a great database with the information, you cannot just pass all to the LLM and say: \"Here is all the information you need. Solve my problems\".\n",
    "\n",
    "The context is limited to 8K tokens or more, depending on the model. If you have a small project, you can pass the whole files directly to LLM and ask questions, but anyway, you will come to the point that you think LLM doesn't understand your project at all.\n",
    "\n",
    "And the solution here is to provide additional information about your project in the context!\n",
    "\n",
    "### SUMMARIZATION\n",
    "\n",
    "That's even more important thing I've learnt. As LLM has limited memory, it cannot remember all the details and facts.\n",
    "\n",
    "Of course, you can write the documentation for the project yourself, comment every line of code and LLM will derive the project's logic out of your comments and documentation.\n",
    "\n",
    "But the nicer way I found is to generate different summaries about the project and pass them to LLM as context before any other code related chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the provided code:\n",
      "\n",
      "**Classes**\n",
      "\n",
      "1. **TemplateWidget**: A base class for all template widgets, responsible for handling JSON data and emitting a signal when something changes.\n",
      "2. **EditTextDialog**: A dialog box for editing text, with an optional \"python\" parameter to enable code editor mode.\n",
      "3. **EditJsonDialog**: A dialog box for editing JSON data, similar to `EditTextDialog` but designed for JSON-specific functionality.\n",
      "4. **LabelTemplateWidget**: A template widget that displays a label, allowing the user to edit its text using an `EditTextDialog`.\n",
      "5. **ButtonTemplateWidget**: A template widget that contains a button, which can be edited or have its command modified using an `EditTextDialog` or other methods.\n",
      "6. **CheckBoxTemplateWidget**: A template widget that contains a checkbox, emitting a signal when the state changes.\n",
      "\n",
      "**Functions**\n",
      "\n",
      "1. `getDefaultData`: Returns the default JSON data for each template widget.\n",
      "2. `getJsonData` and `setJsonData`: Used to get and set JSON data for each template widget.\n",
      "3. `saveAndClose`, `editLabel`, `editCommand`, `buttonClicked`, and `editCommand` methods on various classes, used to handle user interactions and save changes.\n",
      "\n",
      "**Relations**\n",
      "\n",
      "* TemplateWidget -> LabelTemplateWidget, ButtonTemplateWidget, CheckBoxTemplateWidget (inheritance)\n",
      "* TemplateWidget -> EditJsonDialog (related functionality)\n",
      "* ButtonTemplateWidget -> editText Dialog (related functionality)\n",
      "* LabelTemplateWidget -> editText Dialog (related functionality)\n",
      "\n",
      "**Context**\n",
      "\n",
      "The code appears to be part of a larger application or framework for creating and editing templates, possibly related to data exchange between different software systems. The template widgets provide a way to display and edit various types of data, including text, JSON, and commands.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def summarize_python_file(filePath):\n",
    "    with open(filePath) as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    doc = Document(page_content=code)\n",
    "\n",
    "    # split on quite larger chunks as the whole file cannot fit in LLM memory which can lead to strange answers!\n",
    "    code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=7000, chunk_overlap=100)\n",
    "    code_docs = code_splitter.split_documents([doc])\n",
    "\n",
    "    prompt_code_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"In the context are the chunks of the python code. Context: {context}\"),\n",
    "        (\"user\", \"Make a clear and concise summary of the context with classes and functions and their relations.\")\n",
    "    ])\n",
    "\n",
    "    def inspect_state(state):\n",
    "        #print(\"CONTEXT\", state) # in case we want to take a look at the final context\n",
    "        return state\n",
    "\n",
    "    chain = RunnableLambda(inspect_state) | llm # call our function before going to llm\n",
    "    stuff_chain = create_stuff_documents_chain(chain, prompt_code_template) # this create a chain which \"eats\" documents\n",
    "\n",
    "    # summarize each chunk separately\n",
    "    summaries = []\n",
    "    for chunk in code_docs:\n",
    "        response = stuff_chain.invoke({\"context\": [chunk]})\n",
    "        print(response)\n",
    "        summaries.append(response)\n",
    "\n",
    "    # then summarize all the chunks' summaries\n",
    "    summary_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"In the context are the summaries of the chunks of the python code. Context: {context}\"),\n",
    "        (\"user\", \"Make a final summary of the context keeping the logic and relations between the classes and functions.\")\n",
    "    ])\n",
    "\n",
    "    prompt = summary_template.invoke({\"context\": summaries})\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response.content\n",
    "    \n",
    "summary = summarize_python_file(TestFile)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
