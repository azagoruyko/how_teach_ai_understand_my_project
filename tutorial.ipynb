{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083e1eeb",
   "metadata": {},
   "source": [
    "### Problem\n",
    "How to teach AI to understand my python project?  \n",
    "\n",
    "I want to be able to ask AI questions about the project:\n",
    "* Where function X is defined and how it's used in the project?\n",
    "* What's the architecture of the project?\n",
    "* How to add new components?\n",
    "* etc.\n",
    "\n",
    "Below is my experience I'd like to share with you. Hope you find it interesting to start experimenting with LLMs.\n",
    "\n",
    "### Toolset installation\n",
    "\n",
    "##### Python\n",
    "Download and install Python 3.9 or higher. Then install requirements.\n",
    "\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This should install FAISS for vector databases, Langchain for LLM based applications and other dependencies.\n",
    "\n",
    "In case you don't want to clog the system repository, use virtual environment.\n",
    "\n",
    "```console\n",
    "python -m venv python_ai\n",
    "python_ai\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "##### Ollama\n",
    "Ollama is a framework for running LLMs locally.\n",
    "[Download](https://github.com/ollama/ollama/releases) and install it as usual.\n",
    "\n",
    "Once installed, pull LLM model which we are going to use during the tutorial.\n",
    "```console\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "Make sure the model is downloaded with the command:\n",
    "```console\n",
    "ollama list\n",
    "```\n",
    "\n",
    "The last command runs Ollama as server if it's not running yet:\n",
    "```console\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Ollama is ready! It stays in background and provides LLMs for us.\n",
    "\n",
    "## Let's go!\n",
    "Run the following code to make sure everything is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b8aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.1) # the model we want to use, the more temperature the more creative/crazy\n",
    "response = llm.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b3ff",
   "metadata": {},
   "source": [
    "You should get the response from LLM that it's ready for helping you. The cool thing here is that you can pull new models in Ollama and use them right away! \n",
    "\n",
    "You can even use multiple models at the same time.\n",
    "\n",
    "You can browse models on [Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&num_parameters=min:0,max:12B&sort=trending&search=gguf). It's like github for LLMs. Ollama supports GGUF models.\n",
    "\n",
    "Make sure that models can be very large and it directly affects the memory required to load the model and the computation time. \n",
    "\n",
    "**I don't recommend experimenting with models larger than 4 GB as it can be very slow and consume a lot of memory.**\n",
    "\n",
    "Ok, let's define some variables for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d757899",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjectFolder = \"./rigBuilder\" # that's a path to your project, use your own path\n",
    "TestFile = ProjectFolder+\"/widgets.py\" # some file for testing from the project\n",
    "ProjectInfo = \"Rig Builder is a tool for making UIs for python scripts.\" # information about your project for LLM to start understand something\n",
    "Question = \"How to add a custom template widget for my project?\" # the question you want to answer about the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbf025",
   "metadata": {},
   "source": [
    "\n",
    "Here we start working with FAISS. FAISS is a vector database for storing and searching vectors.\n",
    "It converts texts or documents to vectors and stores them in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b724b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface  import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "with open(TestFile) as f:\n",
    "    text = f.read()\n",
    "\n",
    "testFileDocument = Document(page_content=text, metadata={\"source\": TestFile})\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # embeddings model converts text to vector\n",
    "vector_db = FAISS.from_documents([testFileDocument], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90ccc",
   "metadata": {},
   "source": [
    "`Document` is a class from Langchain that represents a textual document. It works as a base class for all documents in Langchain. Metadata is really helpful for filtering documents by type, file name, etc.\n",
    "\n",
    "FAISS requires a model to be used for vectorization. We use here `sentence-transformers/all-MiniLM-L6-v2` model. There are lots of them, but once you've chosen one, you cannot change it for the same vector database. For simplicity, understand this model as a function that converts text to a vector (array of numbers).\n",
    "\n",
    "Then you can search for similar texts in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acf7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: 0\n",
      "from PySide2.QtGui import *\n",
      "from PySide2.QtCore import *\n",
      "from PySide2.QtWidgets import *\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import json\n",
      "import math\n",
      "from .utils import *\n",
      "from .editor import *\n",
      "from .jsonWidget import JsonWidget\n",
      "\n",
      "DCC = os.getenv(\"RIG_BUILDER_DCC\") or \"maya\"\n",
      "\n",
      "if sys.version_info.major > 2:\n",
      "    RootPath = os.path.dirname(__file__) # Rig Builder root folder\n",
      "else:\n",
      "    RootPath = os.path.dirname(__file__.decode(sys.getfilesystemencoding())) # legacy\n",
      "\n",
      "if DCC == \"maya\":\n",
      "    import maya.cmds as cmds\n",
      "\n",
      "class TemplateWidget(QFrame):\n",
      "    somethingChanged = Signal()\n",
      "\n",
      "    def __init__(self, *, executor=None, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.executor = executor # used to execute commands\n",
      "..."
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(vector_db.similarity_search(Question)):\n",
    "    print(f\"CHUNK: {i}\")\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f08c1",
   "metadata": {},
   "source": [
    "The problem here is that we have a single file in the database and so it will always return the same document.\n",
    "\n",
    "Of course, we can add more files to the database and make a searcher that finds a file that's more relevant to the question. But it will anyway return the whole document which is redundant and inefficient, not to mention that it won't give us any relevant information regarding the documents (just whole files).\n",
    "\n",
    "In order to use vector databases more efficiently, we need to split textual data into smaller chunks before storing it. Each chunk is a portion of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccafda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: 0\n",
      "widgets = value[\"widgets\"]\n",
      "        templates = value[\"templates\"]\n",
      "CHUNK: 1\n",
      "for template, d in widgetsData:\n",
      "                templates.append(template)\n",
      "CHUNK: 2\n",
      "layout.addWidget(self.buttonWidget)\n",
      "CHUNK: 3\n",
      "layout.addWidget(self.textWidget)\n",
      "        layout.addWidget(okBtn)\n",
      "CHUNK: 4\n",
      "class TextTemplateWidget(TemplateWidget):\n",
      "    def __init__(self, **kwargs):\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20) # split by text separators and chunk size\n",
    "chunks = text_splitter.split_documents([testFileDocument])\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "for i, chunk in enumerate(vector_db.similarity_search(Question, k=5)): # k is the number of documents to return\n",
    "    print(f\"CHUNK: {i}\")\n",
    "    print(chunk.page_content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b006f",
   "metadata": {},
   "source": [
    "Now we have more documents (pieces of the file) in the database, but the textual fragments as you can see are very small and often look \"random\" to the question. Why?\n",
    "\n",
    "The reason is that the vector database searches for the semantic meaning (similarity). It doesn't understand the code at all, let alone the project structure. \n",
    "\n",
    "If you search for `how to make a new component for my project?` it may find chunks with `def make_component` or `my_project.build()`. You probably don't have such phrases in your codebase like `how to make ...` and so the searching will not give any useful information about how to actually create a new component for the project. You'll just get the nearest code chunks to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469530d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add a custom template widget for your project, you can follow these steps:\n",
      "1. Create a new class that inherits from `TemplateWidget`. This is the base class for all widgets in Rig Builder.\n",
      "2. Define the properties and methods of your custom widget using Python's property decorator and method definitions.\n",
      "3. In the `__init__` method, initialize any attributes or variables specific to your widget.\n",
      "4. Use the `layout.addWidget(self.widget)` method to add your widget to the layout.\n",
      "\n",
      "Here is an example:\n",
      "\n",
      "```python\n",
      "from rigBuilder.widgets import TemplateWidget\n",
      "\n",
      "class CustomTemplateWidget(TemplateWidget):\n",
      "    def __init__(self, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.custom_attribute = kwargs.get('custom_attribute', 'default value')\n",
      "\n",
      "    @property\n",
      "    def custom_property(self):\n",
      "        return f\"Custom property: {self.custom_attribute}\"\n",
      "\n",
      "    def custom_method(self):\n",
      "        print(\"This is a custom method\")\n",
      "```\n",
      "To use this widget in your project, you would need to add it to the `widgetsData` list and then append it to the `templates` list.\n",
      "..."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", ProjectInfo+\"\\nUse the following context to answer the questions. Context:\\n{context}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm # langchain chaining\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5}) # number of chunks to retrieve from vector database\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain) # this is a high level chain for retrieving data somewhere\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": Question}) # user question\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75db094",
   "metadata": {},
   "source": [
    "This answer is not reflecting my project's structure, it won't work. The code above is a bit complicated, let's discover it step by step.\n",
    "\n",
    "First, prompts. A prompt is one of the main components for LLMs. It's very important to understand how it works and affects LLMs. I found that it's more art than science. \n",
    "* Adding adjectives to the prompt can change the output.\n",
    "* Rephrasing the prompt can change the output.\n",
    "* Adding more context to the prompt can change the output.\n",
    "* etc.\n",
    "\n",
    "That's quite unusual for me that a program can work differently just by manipulating words in the prompt.\n",
    "\n",
    "In Langchain, a prompt is just a template with placeholders for variables (sometimes optional). The idea behind prompt templates is that you can define them once and then reuse many times with different variables. \n",
    "\n",
    "A prompt has roles and messages. Briefly:\n",
    "\n",
    "| Role | Message |\n",
    "| -- | -- |\n",
    "| system | You're an expert in AI, answer briefly without long descriptions. |\n",
    "| user | What are the prompts in LLMs? |\n",
    "\n",
    "Second, chaining. The following code creates a _chain_ of operations within Langchain framework. \n",
    "```python\n",
    "chain = prompt_template | llm\n",
    "```\n",
    "\n",
    "Simply, this means that you can pass data into `chain` and it will pass it through `prompt_template` and then through `llm`.\n",
    "\n",
    "Each chain has `invoke` method that actually 'executes' the chain.\n",
    "\n",
    "Important thing here is that `create_retrieval_chain` expects `context` variable to be existed in the prompt.\n",
    "\n",
    "So, how it works?\n",
    "\n",
    "1. You ask a question (it goes to user's `{input}` ).\n",
    "2. `create_retrieval_chain` searches for the nearest chunks to the question in the vector database with the retriever.\n",
    "3. Then passes those chunks to the prompt under `context` variable.\n",
    "4. Then everything is combined and`chain` processes the full prompt with the context.\n",
    "5. `chain` calls LLM and returns the answer.\n",
    "\n",
    "Keys like `answer` and `context` can be changed in the functions call as additional arguments if needed.\n",
    "\n",
    "What do we have after the `chain` call? Did the result get better?\n",
    "\n",
    "If you run the code, you may find that the result is still quite weird and a bit random. \n",
    "\n",
    "Because the actual prompt to the LLM may look like:\n",
    "\n",
    "---\n",
    "```\n",
    "Use the following context to answer the questions. Context:\n",
    "my_component = make_component()\n",
    "    return make_component()\n",
    "    \"\"\"Make a new component for the project.\"\"\"    \n",
    "project.add_component()\n",
    "...\n",
    "how to make a new component for my project?\n",
    "```\n",
    "---\n",
    "LLM gets a mess of code chunks and it's not able to find the correct answer! It doesn't know neither structure of the project, nor the whole code.\n",
    "\n",
    "There are number of solutions to this problem. First, split the file more precisely using dedicated python splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4caf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: 0\n",
      "from PySide2.QtGui import *\n",
      "from PySide2.QtCore import *\n",
      "from PySide2.QtWidgets import *\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import json\n",
      "import math\n",
      "from .utils import *\n",
      "from .editor import *\n",
      "from .jsonWidget import JsonWidget\n",
      "\n",
      "DCC = os.getenv(\"RIG_BUILDER_DCC\") or \"maya\"\n",
      "CHUNK: 1\n",
      "DCC = os.getenv(\"RIG_BUILDER_DCC\") or \"maya\"\n",
      "\n",
      "if sys.version_info.major > 2:\n",
      "    RootPath = os.path.dirname(__file__) # Rig Builder root folder\n",
      "else:\n",
      "    RootPath = os.path.dirname(__file__.decode(sys.getfilesystemencoding())) # legacy\n",
      "\n",
      "if DCC == \"maya\":\n",
      "    import maya.cmds as cmds\n",
      "CHUNK: 2\n",
      "class TemplateWidget(QFrame):\n",
      "    somethingChanged = Signal()\n",
      "\n",
      "    def __init__(self, *, executor=None, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.executor = executor # used to execute commands\n",
      "\n",
      "    def getDefaultData(self):\n",
      "        return self.getJsonData()\n",
      "\n",
      "    def getJsonData(self):\n",
      "        raise Exception(\"getJsonData must be implemented\")\n",
      "CHUNK: 3\n",
      "def getJsonData(self):\n",
      "        raise Exception(\"getJsonData must be implemented\")\n",
      "\n",
      "    def setJsonData(self, data):\n",
      "        raise Exception(\"setJsonData must be implemented\")\n",
      "CHUNK: 4\n",
      "class EditTextDialog(QDialog):\n",
      "    saved = Signal(str) # emitted when user clicks OK\n",
      "\n",
      "    def __init__(self, text=\"\", *, title=\"Edit\", placeholder=\"\", words=None, python=False):\n",
      "        super().__init__(parent=QApplication.activeWindow())\n",
      "\n",
      "        self.setWindowTitle(title)\n",
      "        self.setGeometry(0, 0, 600, 400)\n",
      "\n",
      "        layout = QVBoxLayout()\n",
      "        self.setLayout(layout)\n",
      "CHUNK: 5\n",
      "..."
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=400, chunk_overlap=100) # it will split by python syntax\n",
    "chunks = code_splitter.split_documents([testFileDocument])\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"CHUNK: {i}\")\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad5340",
   "metadata": {},
   "source": [
    "This returns more accurate chunks based on Python syntax. But if you try to run the previous cell, the result won't be much better. You can play with different `chunk_size` and `chunk_overlap` parameters, increase `k` in `as_retriever`, but you will be stuck after some time.\n",
    "\n",
    "This may lead to a question: \"Does LLM really understand what I want?\" because the results are almost random and irrelevant.\n",
    "\n",
    "We've come to one of the most important thing regarding LLMs.\n",
    "\n",
    "#### CONTEXT\n",
    "\n",
    "The context is what LLM is seeing while trying to answer the question. As it's not trained on your project, it doesn't know anything about it. If you pass just code chunks, it will not \"automatically\" create the high-level representation of your project.\n",
    "\n",
    "**Context is limited**. If you have lots of files or a great database, you cannot just pass all to the LLM and say: \"Here is all the information you need. Solve my problems\".\n",
    "\n",
    "The context is limited to 8K tokens or more, depending on the model. If you have a small project, you can pass the whole files directly to LLM and ask questions, but anyway, you will come to the point that you think LLM doesn't understand your project at all.\n",
    "\n",
    "And the solution here is to provide additional information in the context. But rather specific information.\n",
    "\n",
    "#### SUMMARIZATION\n",
    "\n",
    "As LLM has limited memory, it cannot remember all the details and facts.\n",
    "\n",
    "Of course, you can write the documentation for the project yourself, comment every line of code and LLM will derive the project's logic out of your comments and documentation.\n",
    "\n",
    "But the nicer way is to generate different summaries about the project and pass them to LLM as context before any code chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./rigBuilder/widgets.py, CHUNKS COUNT: 10\n",
      "**Architecture Reference**\n",
      "\n",
      "The provided Python code consists of several interconnected components that work together to provide a robust template-based interface.\n",
      "\n",
      "### Core Classes\n",
      "\n",
      "* `TemplateWidget`: A base class for all template widgets, providing a common interface for displaying and editing data.\n",
      "* Specialized template widgets:\n",
      "\t+ `ButtonTemplateWidget`\n",
      "\t+ `CheckBoxTemplateWidget`\n",
      "\t+ `ComboBoxTemplateWidget`\n",
      "\t+ `CurveTemplateWidget`\n",
      "\t+ `JsonTemplateWidget`\n",
      "\t+ `LabelTemplateWidget`\n",
      "\t+ `LineEditTemplateWidget`\n",
      "\t+ `ListBoxTemplateWidget`\n",
      "\t+ `RadioButtonTemplateWidget`\n",
      "\t+ `TableTemplateWidget`\n",
      "\t+ `TextTemplateWidget`\n",
      "\t+ `VectorTemplateWidget`\n",
      "\n",
      "### Data Access and Manipulation\n",
      "\n",
      "* Functions for accessing and manipulating data in templates:\n",
      "\t+ `getDefaultData`\n",
      "\t+ `getJsonData`\n",
      "\t+ `setJsonData`\n",
      "\t+ `smartConversion`\n",
      "\t+ `fromSmartConversion`\n",
      "\n",
      "### Event Handling and Context Management\n",
      "\n",
      "* Functions for handling events related to buttons, text context menus, and options dialogs:\n",
      "\t+ `buttonClicked`\n",
      "\t+ `buttonContextMenuEvent`\n",
      "\t+ `optionsClicked`\n",
      "\t+ `textContextMenuEvent`\n",
      "* Context manager for blocking widget updates: `blockedWidgetContext`\n",
      "\n",
      "### Signal Handling\n",
      "\n",
      "* Signal emitted by the `TemplateWidget` class when data changes: `somethingChanged`\n",
      "\n",
      "### Callbacks\n",
      "\n",
      "* Callbacks for inserting rows and columns into table widgets:\n",
      "\t+ `Callback(self.insertRow, self.tableWidget.currentRow())`\n",
      "\t+ `Callback(self.insertColumn, self.tableWidget.currentColumn())`\n",
      "\n",
      "**Key Features**\n",
      "\n",
      "* The code includes a curve editor with features like Bezier curves, control values (CVs), and mouse event handling.\n",
      "* The `CurveScene` class manages the curve scene and its items, including calculating CVs and drawing background shapes.\n",
      "* The `CurveView` class displays curves using the `CurveTemplateWidget` and `JsonTemplateWidget`.\n",
      "* The `EditCompountWidgetsDialog` class allows editing widgets, including font sizes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def batch(lst, size):\n",
    "    return [lst[i:i + size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def summarize_python_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # split on large chunks as we process them separately, in case of strange results play with chunk_size\n",
    "    code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=8000, chunk_overlap=50)\n",
    "    chunks = code_splitter.split_text(content)\n",
    "    print(f\"FILE: {file_path}, CHUNKS COUNT: {len(chunks)}\")\n",
    "\n",
    "    chunk_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Analyze the chunk code in the context. It's a part of a larger file. Context: {context}\"),\n",
    "        (\"user\", \"Generate a concise, structured architecture summary. List all classes and functions and their purpose, without long descriptions, examples and function arguments.\")\n",
    "    ])\n",
    "\n",
    "    def inspect_state(state):\n",
    "        #print(\"CONTEXT\", state) # when you want to take a look at the final context before sending it to LLM\n",
    "        return state\n",
    "\n",
    "    basic_chain = RunnableLambda(inspect_state) | llm # call our function before going to llm\n",
    "    chunk_chain = chunk_template | basic_chain\n",
    "\n",
    "    # summarize each chunk separately\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        response = chunk_chain.invoke({\"context\": chunk}) \n",
    "        summaries.append(response.content)\n",
    "\n",
    "    # then summarize all the chunks' summaries\n",
    "    summary_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"In the context are the summaries of the chunks of the python code. Context: {context}\"),\n",
    "        (\"user\", \"Combine the summaries from the context into cohesive reference to quickly understand the architecture of the code.\")\n",
    "    ])\n",
    "\n",
    "    summary_chain = summary_template | RunnableLambda(inspect_state) | llm\n",
    "\n",
    "    batch_summaries = []\n",
    "    for summary_list in batch(summaries, 3): # process summaries in batches to avoid context size overflow\n",
    "        summary = \"\\n\\n\".join(summary_list)\n",
    "        response = summary_chain.invoke({\"context\": summary}) # fill the context in the prompt\n",
    "        batch_summaries.append(response.content)\n",
    "\n",
    "    full_summary = \"\\n\\n\".join(batch_summaries)\n",
    "    response = summary_chain.invoke({\"context\": full_summary})\n",
    "    return response.content\n",
    "\n",
    "summary = summarize_python_file(TestFile)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6155d0",
   "metadata": {},
   "source": [
    "The downside of this approach is that it's slow as we have to call LLM many times. But generally I see it's a shamanic dance around the context size.\n",
    "\n",
    "Also try to play with different prompts and you will find that slightly changing \"summary\" to \"knowledge map\" can significantly change the results. LLM is whimsical and fancy.\n",
    "\n",
    "There is a predefined function that works by applying summary logic to chunks and then to the summaries of the chunks recursively. This function is `langchain.chains.summarize.load_summarize_chain`. You can play with it and see if it works better. It also provides different strategies for summarization, like \"map_reduce\" and \"refine\".\n",
    "\n",
    "So, once we have a file summarization function, we can use it to summarize all the project's files and store this information inside our vector database as well as the files themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cf2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./rigBuilder/core.py, CHUNKS COUNT: 5\n",
      "FILE: ./rigBuilder/editor.py, CHUNKS COUNT: 9\n",
      "FILE: ./rigBuilder/jsonWidget.py, CHUNKS COUNT: 4\n",
      "FILE: ./rigBuilder/utils.py, CHUNKS COUNT: 2\n",
      "FILE: ./rigBuilder/widgets.py, CHUNKS COUNT: 10\n",
      "FILE: ./rigBuilder/__init__.py, CHUNKS COUNT: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "from langchain.schema import Document\n",
    "\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=400, chunk_overlap=50) # for code\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50) # for summaries\n",
    "\n",
    "documents = []\n",
    "for file in os.listdir(ProjectFolder):    \n",
    "    if file.endswith(\".py\"):\n",
    "        file_path = ProjectFolder + \"/\" + file\n",
    "\n",
    "        with open(file_path) as f:\n",
    "            content = f.read()\n",
    "\n",
    "        code_doc = Document(page_content=content, metadata={\"source\": file, \"type\":\"code\"})\n",
    "        documents += code_splitter.split_documents([code_doc])\n",
    "\n",
    "        summary = summarize_python_file(file_path)\n",
    "\n",
    "        summary_doc = Document(page_content=summary, metadata={\"source\": file, \"type\":\"summary\"})\n",
    "        documents += text_splitter.split_documents([summary_doc])\n",
    "\n",
    "vector_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vector_db.save_local(\"vector_db\") # we can save it to disk\n",
    "#vector_db = FAISS.load_local(\"vector_db\", embeddings, allow_dangerous_deserialization=True) # and load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934e65d",
   "metadata": {},
   "source": [
    "Once the vector database is filled with the summaries and code chunks, you can use it to answer questions about the project. And it will answer much better (I hope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adf5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add a custom template widget for your project, you can follow these steps:\n",
      "\n",
      "1. **Create a new class**: In the `widgets.py` file, create a new class that inherits from `TemplateWidget`. This will be your custom template widget.\n",
      "\n",
      "2. **Implement the necessary methods**: Your class should implement the following methods:\n",
      "   - `__init__`: Initializes the widget with any necessary parameters.\n",
      "   - `setJsonData`: Sets the JSON data for the widget.\n",
      "   - `getDefaultData`: Returns the default data for the widget.\n",
      "   - `template`: A property that returns the template name.\n",
      "\n",
      "3. **Create a layout**: In the `__init__` method, create a layout (e.g., QVBoxLayout or QGridLayout) and set it as the main layout for your widget.\n",
      "\n",
      "4. **Add widgets to the layout**: Add any necessary widgets to the layout, such as labels, buttons, or text editors.\n",
      "\n",
      "5. **Connect signals and slots**: Connect any signals (e.g., button clicks) to slots (e.g., functions that update the data).\n",
      "\n",
      "6. **Register the custom widget**: In your `TemplateWidgets` dictionary, add a new key-value pair with the name of your custom widget as the value.\n",
      "\n",
      "Here's an example:\n",
      "```python\n",
      "class MyCustomWidget(TemplateWidget):\n",
      "    def __init__(self, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.layout = QVBoxLayout()\n",
      "        self.setLayout(self.layout)\n",
      "\n",
      "        # Add a label and a text editor to the layout\n",
      "        self.label = QLabel(\"My Custom Widget\")\n",
      "        self.textEditor = QTextEdit()\n",
      "        self.layout.addWidget(self.label)\n",
      "        self.layout.addWidget(self.textEditor)\n",
      "\n",
      "    def setJsonData(self, data):\n",
      "        # Update the text editor with the JSON data\n",
      "        self.textEditor.setText(data[\"text\"])\n",
      "\n",
      "    def getDefaultData(self):\n",
      "        return {\"text\": \"Default text\"}\n",
      "\n",
      "    @property\n",
      "    def template(self):\n",
      "        return \"my_custom_template\"\n",
      "\n",
      "# Register the custom widget in the TemplateWidgets dictionary\n",
      "TemplateWidgets[\"my_custom_template\"] = MyCustomWidget\n",
      "```\n",
      "With these steps, you've added a custom template widget to your project.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", ProjectInfo+\"\\nAnswer the user's questions about the project. Context: {context}\"),\n",
    "    (\"human\", \"Question: {input}\")\n",
    "])\n",
    "chain = prompt_template | llm # langchain chaining\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 10}) # number of chunks to retrieve\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain) # this is a high level chain for retrieving data somewhere\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": Question}) # user query\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1adbc",
   "metadata": {},
   "source": [
    "That's much more relevant to my project, but truth be told, it continues to lie!\n",
    "\n",
    "It's good to add another kind of summaries, like a brief description of each file or relationship between classes, which can help LLM to understand the project better. A basic project description written manually can also be very helpful. Truth be told, I've found that those summaries are the key point for LLMs to understand the complex structures due to their restricted memory.\n",
    "\n",
    "Langchain and FAISS provide lots of tools and high level functions to manipulate vector databases and chains, like removing documents or filtering them by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9453f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.similarity_search(\"What is the main purpose of this project?\", filter={\"type\":\"summary\"}) # search for summaries\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 7, \"filter\": {\"type\":\"summary\"}}) # create retriever with filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286b075",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "LLM still doesn't understand the project as it a human does. That's my conclusions.\n",
    "\n",
    "* Prompts are very important and affect the overal quality of the response. It's more than it seems.\n",
    "* LLMs context is limited, but you have to provide as much as possible to LLM to understand the problem.\n",
    "* Summarize everything and store this information inside vector databases as well as the code chunks.\n",
    "* The lie is in its nature!\n",
    "\n",
    "Hope you found this tutorial helpful and intriguing for your further researches. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
